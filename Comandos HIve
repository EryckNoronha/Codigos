Para iniciar:beeline
conectar: !connect jdbc:hive2://

para sair:!q

criar Data base: CRATE DATA BASE NOME_DATABAS;
criar tabela: CREATE EXTERNAL TABLE NOME_TABELA(
              nome int,
              nome string,
              nome date )
              row format delimited fields terminated by ',' STORED AS TEXTFILE;
 
inserir dados: LOAD DATA INPATH '/user/caminho/caminho/nomeArquivo.csv' INTO TABLE CLIENTES;
inserir um campo: insert into nomeTabela values( 11, 'valor1','valor2', ...);
ver dados: select * from nomeTabela;
ver tabelas: show tables;
descrição tabelas: describe nomeTabela;
informações da estrutura da tabela: describe formatted nomeTablea;
informações banco de dados: DESCRIBE DATABASE nomeDataBase;

--------------------OUTRA MANEIRA DE ACESSAR DADOS HIVE-----------------------
consultando Hcatalog no linux
Conectando ao MySQL
mysql -u root -pcloudera
obs. metastore é onde tem as informações de catalogo do hive

ver os bancos para ter acesso ao id do database: select * from DBS;
acessar database: select * from TBLS where DB_ID=numero_do_id_database;
acessar tabelas: SELECT * FROM COLUMNS_V2 WHERE CD_ID = (NUMERO DA COLUNA *TBL_id)
----------------------------------------------------------------------------------

selecionar dados: select * from nomeTabela;
selecionar algumas colunas: select coluna1, coluna2, coluna3 from nomeTabela;
valores unicos coluna: select distinct nomeColuna from nomeTabela;
filtro na linha: select * from nomeTabela where nomeColune operadorLogico(ex. <> (diferente)) "valor do campo";

mais de uma condição: select * from nomeTabela where nomeColune operadorLogico(ex. <> (diferente)) "valor do campo" and nomeColune operadorLogico(ex. <> (diferente)) "valor do campo"
  TODAS AS CONDIÇÔES TEM QUE SER ATENDIDAS--->>> ex. select * from veiculos where status = "Disponivel" and diaria >= 1600;

Ou uma ou outra condição: select * from nomeTabela where nomeColune operadorLogico(ex. <> (diferente)) "valor do campo" or nomeColune operadorLogico(ex. <> (diferente)) "valor do campo"
  AL MENOS UMA DAS CONDIÇÔES TEM QUE SER ATENDIDAS--->>> ex. select * from veiculos where status = "Disponivel" or diaria >= 1600
ordenar: select * from nomeTable order by nomeColuna1(ordera primeiro), nomeColuna2
         obs decrescente: select * from nomeTable order by nomeColuna1 desc

limete de registros: select * from locacao limit 5;
ex. select * from locacao order by dataLocacao limit 5

Agregação: select max(total) from nomeTable;
         obs no lugar do mas pode ser: min, count, avg, sum ...
         
pesquisa texto: select * from nomeTabel where nomeColune like 'ABC%' --> qualquer coisa no lado direito
                                                              '%ABC%' --> em algum lugar tenha 
                                                              '%ABC' --> qualquer coisa esquerda
                                                             
clausula de conjunto: select * from nomeTable where nomeColuna in ('valor1','valor2');
intervalo: select * from nomeTable where nomeColuna between 1000 and 2000;

Junções
INNER JOIN - SOMENTE A INTERCEÇÃO
LEFT JOIN - ESQUERDA TUDO DIREITA SÓ EQUIVALENCIA
RIGHT JOIN - DIREITA TUDO ESQUERDA SÓ EQUIVALENCIA
FULL JOIN - JUNTA TUDO

select loc.coluna1,descoluna2 from locacao loc (<-apelido)  
join despachante des on (<-apelido) on (loc.id = des.id);

select loc.coluna1,descoluna2 from locacao loc (<-apelido)  
right join despachante des on (<-apelido) on (loc.id = des.id);

Desnormalização (Juntar varias tabelas)

select loc.coluna1,des.coluna2,cli.coluna4,veic.coluna5 from locacao loc (<-apelido)  
join despachante des(<-apelido) on (loc.id = des.id)
join clientes cli(<-apelido) on (loc.id = cli.id)
join veiculo veic(<-apelido) on (loc.id = veic.id);

Agrupando com valores unicos e somando valores

select veic.modelo, sum(loc.total) from locacao loc
join veiculos veic on (loc.id = veic.id)
group by veic.modelo;

select veic.modelo,desp.nome, AVG(loc.total) from locacao loc
join veiculos veic on (loc.id = veic.id)
join despachante desp on (loc.id=desp.id)
group by veic.modelo, desp.nome;

Condição para agregação (no final do group by

select veic.modelo,desp.nome, AVG(loc.total) from locacao loc
join veiculos veic on (loc.id = veic.id)
join despachante desp on (loc.id=desp.id)
group by veic.modelo, desp.nome having sum(loc.total) > 10000;

Buscar mes e ano especificos
select veic.modelo,desp.nome, sum(loc.total) from locacao loc
join veiculos veic on (loc.id = veic.id)
join despachante desp on (loc.id=desp.id)
where month(loc.datalocacao) = 2 and year(loc.datalocacao) = 2019  ----> month busca mes, year busca ano
group by veic.modelo, desp.nome

Montando coluna mes e ano
select veic.modelo,desp.nome, month(loc.datalocacao) Mes, year(loc.datalocacao) ANO from locacao loc
join veiculos veic on (loc.id = veic.id)
join despachante desp on (loc.id=desp.id)
SITE COM COMANDOS HIVE: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF


-----------------Montando Sqoop---------------
listar database
sqoop list-databases --connect jdbc:mysql://localhost/ --username root --password cloudera

listar tabelas
sqoop list-tables --connect jdbc:mysql://localhost/retail_db --username root --password cloudera

IMPORTANDO DO SQOOP PARA HIVE
cria database no hive: create database retail_db;
no bash: sqoop import-all-tables --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --hive-import --hive-overwrite --hive-database retail_db --create-hive-table --m 1

Verificando no MySQL se a tranferencia foi bem sucedida
apos conectar no MySQL
select (*) from order_items apos o mesmo select no hive para conferir

Importação incremental, inserindo dados no MySQL e atualisando com Sqoop
no MySQL: insert into nomeTablea values (1, 2, 'texto');
importando
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --hive-import --hive-database retail_db --check-colun nomeColuna --incremental append --last-value numeroUltimoRegistro --table NomeTabela

---------------Salvando no HDFS---------------------
insert overwrite directory '/user/cloudera/NomeNovo' select * from nomeTabela;
verificando no bash
hdfs dfs -ls /user/cloudera/NomeNovo (será listado)
para visualizar o conteudo
hdfs dfs -cat /user/cloudera/NomeNovo/00000-00 (o caminho foi mostrado com o comnado acima)
obs. o conteudo não pode ser lido por humano

Salvar local
insert overwrite local directory '/home/cloudera/NomeNovo' row format delimited fields terminated by ',' select * from nomeTabela
verificando no bash
ls '/home/cloudera/NomeNovo'
ver o conteudo
cat /user/cloudera/NomeNovo/00000-00 (o caminho foi mostrado com o comnado acima)

------------ Criando Partição ---------------------------------------------------------
ver configuração do paramnetro dynamic
set hive.exec.dynamic.partition.mode; (obs. já devo estar usando o database no modo hive2)

obs. se mode=strict tenho que mudar para mode=nonstrict
mudando: set hive.exec.dynamic.partition.mode=nonstrict;

criando tabela particionada:
crate table NomeNovo (nome1 string, nome2 date, nome3 string, nome4 double) PARTITIONED BY (colunaParticionar string);

inserindo dados:
INSERT OVERWRITE TABLE NomeNovo PARTITIONED (colunaParticionar)
select cli.nome1, des.nome2, loc.nome3, veic.nome4
from locacao loc
join despachante des on (loc.id = des.id)
join clientes cli on (loc.id = cli.id)
join veiculos veic on (loc.id = veic.id);

-----------Criando Bucketing -----------------------

criando tabela particionada:
crate table NomeNovo (nome1 string, nome2 date, nome3 string, nome4 double)
clustered by (colunaParticionar) into 4 buckets;


inserindo dados:
INSERT OVERWRITE TABLE NomeNovo 
select cli.nome1, des.nome2, loc.nome3, veic.nome4
from locacao loc
join despachante des on (loc.id = des.id)
join clientes cli on (loc.id = cli.id)
join veiculos veic on (loc.id = veic.id);
_-------------------------------------------------------------

criando tabela temporaria
crate temporary table nomeTabelaTemporaria as select * tabela;

criando viw (atalho para consulta tabela)
create viw if not exists nomeTbelaViw as
select + joins e etc;

----------------------ORC------------------------
criando tabela
CREATE EXTERNAL TABLE
TABLEAorc (nome1 string, nome2 date, nome3 string, nome4 double)
stored as orc;

inserindo dado
insert overwrite table TABELAorc select * from tabelaCopia;

configurações se quizer padrão:
hive.default.fileformat  ---- padrão:TextFile ---- opição: ORC
hive.exec.orc.default.compress --- padrão: ZLIB --- opção: NONE,ZLIB,SNAPPY
orc.compress --- padrão: ZLIB --- opção: NONE,ZLIB,SNAPPY
orc.crate.index --- padrão: true --- opção --- true, false








