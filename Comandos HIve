
Os tipos de dados do Hive incluem
1. Primitive Types:
    a. Booleans (BOOLEAN)
    b. Integers (TINYINT, SMALLINT, INT, BIGINT, FLOAT, DOUBLE)
    c. Strings (STRING, VARCHAR, CHAR)
    d. Dates (DATE, TIMESTAMP)

2. Complex Types:
    a. Arrays (ARRAY<DATA_TYPE>) 
    b. Maps (MAP<KEY_DATA_TYPE, VALUE_DATA_TYPE>)
    c. Structs (STRUCT<Data_type, Data_type2, ...>) 
    d. Union (UNIONTYPE<DATA_TYPE1, DATA_TYPE2, ...>)


Para iniciar:beeline
conectar: !connect jdbc:hive2://

para sair:!q

criar Data base: CRATE DATA BASE NOME_DATABAS;

criar tabela: CREATE EXTERNAL TABLE NOME_TABELA(
              nome int,
              nome string,
              nome date )
              row format delimited fields terminated by ',' STORED AS TEXTFILE;
outro exemplo
criar tabela: CREATE EXTERNAL TABLE NOME_TABELA(
              nome int,
              nome string,
              nome date )
              row format delimited 
                 fields terminated by '|' --> separado por pip
                 collection items terminated by ':' --> coleção na matriz são terminados com :
              STORED AS TEXTFILE;
              
criando tabvela em um diretorio:
CREATE EXTERNAL TABLE user
LOCATION '/local1/local2/';

apagar tabela:
DROP TABLE IF EXISTS user;

mudar nome:
ALTER TABLE user RENAME TO employees;

adicionar coluna
ALTER TABLE user  ADD COLUMNS (
nome1 string,
nome2 int);


inserir dados: LOAD DATA INPATH '/user/caminho/caminho/nomeArquivo.csv' INTO TABLE CLIENTES;
inserir um campo: insert into nomeTabela values( 11, 'valor1','valor2', ...);
ver dados: select * from nomeTabela;
ver tabelas: show tables;
descrição tabelas: describe nomeTabela;
informações da estrutura da tabela: describe formatted nomeTablea;
informações banco de dados: DESCRIBE DATABASE nomeDataBase;

------------INDEX----------------------
criar:
CREATE INDEX table01_index ON TABLE table_01 (column2) as 'COMPACT'

ver: 
SHOW INDEX ON table01

apagar:
DROP INDEX table01_index ON table_01

--------------------OUTRA MANEIRA DE ACESSAR DADOS HIVE-----------------------
consultando Hcatalog no linux
Conectando ao MySQL
mysql -u root -pcloudera
obs. metastore é onde tem as informações de catalogo do hive

ver os bancos para ter acesso ao id do database: select * from DBS;
acessar database: select * from TBLS where DB_ID=numero_do_id_database;
acessar tabelas: SELECT * FROM COLUMNS_V2 WHERE CD_ID = (NUMERO DA COLUNA *TBL_id)
----------------------------------------------------------------------------------

selecionar dados: select * from nomeTabela;
selecionar algumas colunas: select coluna1, coluna2, coluna3 from nomeTabela;
valores unicos coluna: select distinct nomeColuna from nomeTabela;
filtro na linha: select * from nomeTabela where nomeColune operadorLogico(ex. <> (diferente)) "valor do campo";

mais de uma condição: select * from nomeTabela where nomeColune operadorLogico(ex. <> (diferente)) "valor do campo" and nomeColune operadorLogico(ex. <> (diferente)) "valor do campo"
  TODAS AS CONDIÇÔES TEM QUE SER ATENDIDAS--->>> ex. select * from veiculos where status = "Disponivel" and diaria >= 1600;

Ou uma ou outra condição: select * from nomeTabela where nomeColune operadorLogico(ex. <> (diferente)) "valor do campo" or nomeColune operadorLogico(ex. <> (diferente)) "valor do campo"
  AL MENOS UMA DAS CONDIÇÔES TEM QUE SER ATENDIDAS--->>> ex. select * from veiculos where status = "Disponivel" or diaria >= 1600
ordenar: select * from nomeTable order by nomeColuna1(ordera primeiro), nomeColuna2
         obs decrescente: select * from nomeTable order by nomeColuna1 desc

limete de registros: select * from locacao limit 5;
ex. select * from locacao order by dataLocacao limit 5

Agregação: select max(total) from nomeTable;
         obs no lugar do mas pode ser: min, count, avg, sum ...
         
pesquisa texto: select * from nomeTabel where nomeColune like 'ABC%' --> qualquer coisa no lado direito
                                                              '%ABC%' --> em algum lugar tenha 
                                                              '%ABC' --> qualquer coisa esquerda
                                                             
clausula de conjunto: select * from nomeTable where nomeColuna in ('valor1','valor2');
intervalo: select * from nomeTable where nomeColuna between 1000 and 2000;

Junções
INNER JOIN - SOMENTE A INTERCEÇÃO
LEFT JOIN - ESQUERDA TUDO DIREITA SÓ EQUIVALENCIA
RIGHT JOIN - DIREITA TUDO ESQUERDA SÓ EQUIVALENCIA
FULL JOIN - JUNTA TUDO

select loc.coluna1,descoluna2 from locacao loc (<-apelido)  
join despachante des on (<-apelido) on (loc.id = des.id);

select loc.coluna1,descoluna2 from locacao loc (<-apelido)  
right join despachante des on (<-apelido) on (loc.id = des.id);

Desnormalização (Juntar varias tabelas)

select loc.coluna1,des.coluna2,cli.coluna4,veic.coluna5 from locacao loc (<-apelido)  
join despachante des(<-apelido) on (loc.id = des.id)
join clientes cli(<-apelido) on (loc.id = cli.id)
join veiculo veic(<-apelido) on (loc.id = veic.id);

Agrupando com valores unicos e somando valores

select veic.modelo, sum(loc.total) from locacao loc
join veiculos veic on (loc.id = veic.id)
group by veic.modelo;

select veic.modelo,desp.nome, AVG(loc.total) from locacao loc
join veiculos veic on (loc.id = veic.id)
join despachante desp on (loc.id=desp.id)
group by veic.modelo, desp.nome;

Condição para agregação (no final do group by

select veic.modelo,desp.nome, AVG(loc.total) from locacao loc
join veiculos veic on (loc.id = veic.id)
join despachante desp on (loc.id=desp.id)
group by veic.modelo, desp.nome having sum(loc.total) > 10000;

Buscar mes e ano especificos
select veic.modelo,desp.nome, sum(loc.total) from locacao loc
join veiculos veic on (loc.id = veic.id)
join despachante desp on (loc.id=desp.id)
where month(loc.datalocacao) = 2 and year(loc.datalocacao) = 2019  ----> month busca mes, year busca ano
group by veic.modelo, desp.nome

Montando coluna mes e ano
select veic.modelo,desp.nome, month(loc.datalocacao) Mes, year(loc.datalocacao) ANO from locacao loc
join veiculos veic on (loc.id = veic.id)
join despachante desp on (loc.id=desp.id)
SITE COM COMANDOS HIVE: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF

Apelidando consulta:
FROM(
     SELECT coluna1, round(coluna2 * .01) as renomeColuna2
     FROM tablea;
) c
SELECT c.coluna1, c.coluna2
WHERE c.renomeColuna2 > 25;

fazendo cunsulta mudando o tipo da coluna
SELECT coluna1 , coluna2
FROM tabela
WHERE cast(colunaParaMudar AS FLOAT) > 100;

-----------------Montando Sqoop---------------
listar database
sqoop list-databases --connect jdbc:mysql://localhost/ --username root --password cloudera

listar tabelas
sqoop list-tables --connect jdbc:mysql://localhost/retail_db --username root --password cloudera

IMPORTANDO DO SQOOP PARA HIVE
cria database no hive: create database retail_db;
no bash: sqoop import-all-tables --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --hive-import --hive-overwrite --hive-database retail_db --create-hive-table --m 1

Verificando no MySQL se a tranferencia foi bem sucedida
apos conectar no MySQL
select (*) from order_items apos o mesmo select no hive para conferir

Importação incremental, inserindo dados no MySQL e atualisando com Sqoop
no MySQL: insert into nomeTablea values (1, 2, 'texto');
importando
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --hive-import --hive-database retail_db --check-colun nomeColuna --incremental append --last-value numeroUltimoRegistro --table NomeTabela

---------------Salvando no HDFS---------------------
insert overwrite directory '/user/cloudera/NomeNovo' select * from nomeTabela;
verificando no bash
hdfs dfs -ls /user/cloudera/NomeNovo (será listado)
para visualizar o conteudo
hdfs dfs -cat /user/cloudera/NomeNovo/00000-00 (o caminho foi mostrado com o comnado acima)
obs. o conteudo não pode ser lido por humano

Salvar local
insert overwrite local directory '/home/cloudera/NomeNovo' row format delimited fields terminated by ',' select * from nomeTabela
verificando no bash
ls '/home/cloudera/NomeNovo'
ver o conteudo
cat /user/cloudera/NomeNovo/00000-00 (o caminho foi mostrado com o comnado acima)

------------ Criando Partição ---------------------------------------------------------
ver configuração do paramnetro dynamic
set hive.exec.dynamic.partition.mode; (obs. já devo estar usando o database no modo hive2)

obs. se mode=strict tenho que mudar para mode=nonstrict
mudando: set hive.exec.dynamic.partition.mode=nonstrict;

criando tabela particionada:
crate table NomeNovo (nome1 string, nome2 date, nome3 string, nome4 double) PARTITIONED BY (colunaParticionar string);

inserindo dados:
INSERT OVERWRITE TABLE NomeNovo PARTITIONED (colunaParticionar)
select cli.nome1, des.nome2, loc.nome3, veic.nome4
from locacao loc
join despachante des on (loc.id = des.id)
join clientes cli on (loc.id = cli.id)
join veiculos veic on (loc.id = veic.id);

-----------Criando Bucketing -----------------------

criando tabela particionada:
crate table NomeNovo (nome1 string, nome2 date, nome3 string, nome4 double)
clustered by (colunaParticionar) into 4 buckets;


inserindo dados:
INSERT OVERWRITE TABLE NomeNovo 
select cli.nome1, des.nome2, loc.nome3, veic.nome4
from locacao loc
join despachante des on (loc.id = des.id)
join clientes cli on (loc.id = cli.id)
join veiculos veic on (loc.id = veic.id);
_-------------------------------------------------------------

criando tabela temporaria
crate temporary table nomeTabelaTemporaria as select * tabela;

criando viw (atalho para consulta tabela)
create viw if not exists nomeTbelaViw as
select + joins e etc;

----------------------ORC------------------------
verse a vetorização esta desabilitada:
set hihve.vextorized.execution

criando tabela
CREATE EXTERNAL TABLE
TABLEAorc (nome1 string, nome2 date, nome3 string, nome4 double)
stored as orc;

inserindo dado
insert overwrite table TABELAorc select * from tabelaCopia;

configurações se quizer padrão:
hive.default.fileformat  ---- padrão:TextFile ---- opição: ORC
hive.exec.orc.default.compress --- padrão: ZLIB --- opção: NONE,ZLIB,SNAPPY
orc.compress --- padrão: ZLIB --- opção: NONE,ZLIB,SNAPPY
orc.crate.index --- padrão: true --- opção --- true, false


ver se a vetorização está desabilitada
set hive.vectorized.execution.enable; se = false está desabilitado coloco = true para habilitar

---------Habilitar CBO----------------
Dados estatisticos do banco
set hive.cbo.enable=true;
set hive.compute.query.using.stats=true;
set hive.stats.fetch.column.stats=tru;
set hive.stats.fetch.partition.stats=true;
obs. para voltar coloca tudo false.

se as tabelas já foram criadas é preciso calcular suas estatisticas
ANALYZE TABLE LOCACAO_ORC COMPUTE STATISTICS;
ANALYZE TABLE LOCACAO_ORC COMPUTE STATISTICS FOR COLUMNS; está com bug tem que ver

--------------Spark--------------------

iniciando a configuração nivel local
set hive.execution.engina; -- para ver como está
set hive.execution.engina=spark; -- para colocar como spark
set hive.execution.engina=mr; -- para voltar para mapreduce

a nivel de servidor tem que alterar
sudo gedit /etc/hive/conf.dist/hive-site.xml -- acessa

<property>
    <name>hive.execution.engina</name>
    <value>spark</value>
</property>

reiniciar hive
sudo service hive-server2 stop
sudo service hive-server2 start
--




