https://github.com/stacktecnologias/stack-repo/blob/master/PySpark_Aula_01.ipynb
https://github.com/Pedrocanizel/Pedrocanizel/blob/main/pyspark_aula_03.ipynb
https://github.com/Pedrocanizel/Pedrocanizel

https://www.youtube.com/watch?v=EMACjF6eCU4&t=45s

#ler df com spark
df1 = spark.read.csv('/content/df_space.csv', sep=';', header=True)

transformar em df pandas
dfPandas = df1.toPandas()

----------------------------------
Select
--------------------------------
#filtrando por condição coluna
df.filter("nome_coluna > 120").show()


# Definindo o primeiro filtro
filterA = df.origin == "SEA"

# Definindo o segundo filtro
filterB = df.dest == "PDX"

# Filtrando os dados, primeiro pelo filterA entao pelo filterB
selected2 = selected2.filter(filterA).filter(filterB)

#selecionar item na coluna
df1.select("name").filter(F.col('id')==6077).collect()


#vendo condição when = quando , otherwise=caso contrario
df2 = (df1.withColumn('nome_coluna_criar', F.when(F.col('nome coluna')>0, 'Tem casos novos') .otherwise('Nao tem casos novos'))) 

#concatenando valor e escrevendo juntos na coluna
f3 = (df1.withColumn("nome coluna1", F.when(F.col("nome coluna2") > 0, F.concat(df1.nome coluna2,F.lit(" casos novos"))) .otherwise("Não tem casos novos")))

#criar coluna pegando pedações da string
#substring('nome coluna', posição index str(começa por 1), quantidade que vai pegar))
df4 = df1.withColumn('ano', F.substring('data', 1, 4)).withColumn('mes', F.substring('data', 6, 2)).withColumn('dia', F.substring('data', 9, 2))

------------------------
Agregar
------------------------------
# Achar a maior tempo de voo de SEA para outras cidades
df.filter(df.nome_coluna== "o que busca").groupBy().max("nome_coluna2").show()


# Achar a menor distancia do voo de PDX para outras cidades
ds.filter(df.origin == "PDX").groupBy().min("nome_coluna2").show()


# Duração Média dos Voos da compania delta
df.filter(df.nome_coluna == "DL").filter(flights.origin == "SEA").groupBy().avg("nome_coluna2").show()

# Tempo total em Horas no ar 
df.withColumn("nome_coluna", flights.air_time/60).groupBy().sum("nome_coluna2").show()

--------------------
Visualizar
-------------------
#agrupando e vendo media
das pessoas media = df.select(F.mean('idade')).collect()[0][0] --> [0] linha1, [0]--> coluna 1
df.groupBy().mean('idade').show()

#visualizar datasets disponiveis em diretório
display(dbutils.fs.ls("caminho"))

#ler o arquivo
arquivo  = "caminho" -> transformou em uma str

#criando arquivo
df = spark\ 
.read.format("csv")\
.opition("interSachema", "True")\ #interSachema escolhe altomatico os tipos dos dados para trabalhar
.opitions("header","True")\ #tem cabeçalho
.csv(arquivo) #localisação dos arquivos

#alterar tipo coluna com cast
df5 = (
    df1.withColumn('ano', F.substring('data', 1, 4).cast('integer'))
    .withColumn('mes', F.substring('data', 6, 2).cast('integer'))
    .withColumn('dia', F.substring('data', 9, 2).cast('integer'))
    )

#print
df1.printSchema()
df1.show(n=5, truncate= False)

#tipo
type('nome df')

#ver as 5 primeiras linhas no formato array
df.take(numero de linhas)

#quantidade de linhas
df.count()

CONSULTANDO DADOS
#
from pyspark.sql.functions import max
df.select(max("nome coluna")).take(1) -> retorna o maior valor da coluna


#filtrar linha
df.filter(""nome coluna < valor").show(quantidade de linhas que querto) 

df.where(""nome coluna < valor").show(quantidade de linhas que querto) 

#ordernar pela coluna
df.sort("nome coluna").show(quantidade de linhas que querto) 

from pyspark.sql.functions import desc, asc, expr

# ordernar por ordem crescente
df.orderBy(expr("nome coluna desc")).show(quantidade de linhas que querto)

#estatisticas descritivas
df.describe().show()

#fazendo for interando linhas com collect()
for i in df.collect():
     i -> retorna toda a linha
     i[index] -> retorna o conteudo

-------------------------------
Colunas
-------------------------

#adicionar nova coluna
df = df.withColumn('Nova coluna',dados a receber)

#criando nova coluna
df2 = df.withColumn("duration_hrs", df.nome_coluna/60)

#renomeou
df.select((df.nome_coluna/60).alias("novo_nome")).show()


# Selecionando um subconjunto do dataset
selected1 = flights.select("tailnum", "origin", "dest")
selected1.show(5)

#remover coluna
df = df.drop('nome da coluna')

#Retirando as datas e passando colunas para tipos corretos.
flights = flights.\
        withColumn("new_air_time", col("air_time").cast("integer")).drop("air_time")

#renomear coluna
df.withColumnRenamed('Nome de Coluna','Novo nome').show()

#llistando colunas desejadas
lista = ["tailnum", "origin", "dest"]
selected2 = df.select(lista)
selected2.show(5)


---------------------------------------------------------
#filtrar valores nulos
df.filter("nome da coluna NULL").show()



#filtrar valores missing
df.filter(df.nomecoluna.isNull()).show()

#substituir os valos NAN por 
df.na.fill(value=valor).show()

#subtituir os valores nan na coluna especifica
df.na.fill(value=valor, subset=['nome coluna']).show()

#remover linhas com valores vazio na
df.na.drop().show()

#salvar como tabela
df.write.mode('overwrite') \
    .saveAsTable("df5")
---------------------------------------------------------
Temporay e  global Viw
-------------------------------------------------
#Registrando o dataframe em uma view temporária
df.createOrReplaceTempView("nome_df")

query = "FROM nome_df SELECT * LIMIT 10"

# Selecionando as 10 primeiras linhas do dataset
variavel = spark.sql(query)


# Registtrando o dataframe como view global
flights.createGlobalTempView("flights")

# A visão temporária global está vinculada a um banco de dados preservado pelo sistema `global_temp`
spark.sql("SELECT * FROM global_temp.flights LIMIT 10").show()
_______________________________

--------------------
Passando para pandas
--------------------
query = "SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest"

# Rodando a query
flight_counts = spark.sql(query)
import pandas as pd     

# Convertendo o resultado para pandas
df = flights.toPandas()

# df pandas para spark
spark_temp = spark.createDataFrame(df)

------------------
Join
-----------------

# Join os DataFrames
flights_with_airports = df1.join(df2, on="dest", how="leftouter") --> on: é o que os dfs tem de semelhante, how: tipo do join






